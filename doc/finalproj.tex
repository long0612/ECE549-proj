
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
\linespread{1.2}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
\usepackage{multirow}
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{relsize}
\usepackage[cmex10]{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithm}
 \usepackage{algpseudocode} 
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{theorem}{Theorem}[subsection]%[section]
\newtheorem{definition}{Definition}[subsection]%[section]
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{ECE 549 Final Project: Video Google - Image Search in Videos.}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Long Le and Amey Chaugule}
\IEEEauthorblockA{Department of Electrical and Computer Engineering\\
University of Illinois at Urbana-Champaign}%}
%Atlanta, Georgia 30332--0250\\
Email: longle1@illinois.edu and achaugu2@illinois.edu}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
This project proposes to implement a system that finds all relevant frames for a given query image in videos, hence 
the name Video Google. It uses the algorithm developed by Sivic and Zisserman in \cite{sivic2003video}, with minor modifications. 
The basic algorithm consists of the following steps. First, feature detection and descriptions are done independently for each frame in a video.
Then a tracking mechanism based on a simple video dynamical model and correlation is used to pruned out many noisy descriptors across frames. 
The surviving
descriptors are further clustered into visual words using K-means with Mahalanobis distance, effectively converts the image retrieval 
problem into the well-known document retrieval problem. Therefore the standard term frequency-inverse document frequency (tf-idf) statistic  can
be used to represented each image for the ranking and retrieval purpose. The retrieval performance is evaluated using the average normalized 
retrieval rank (ANRR) criterion. Our system achieves a worst case ANRR of 0.3677 on a video clip from the movie Charade. Furthermore, since
an image object is simply a sub-part of an image, the system is also demonstrated to retrieve objects in videos with relatively high confidence.

\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


% Need to cite at least one reference for the compilation to be successfull.

\section{Introduction}
% no \IEEEPARstart
\label{sec:intro}
Large amount of video data are being generated every day, especially for surveillance and monitoring applications. 
Searching for interesting objects in such big video data manually is a time consuming task. This motivates the need to automate image 
search in videos. Specifically, one would like a system that, given a query image, responses with a list of the most relevant frames in a video.
An implementation of such a system, dubbed Video Google for the apparent reason, is the goal of this project.

An algorithm for Video Google was developed by Sivic and Zisserman in \cite{sivic2003video}. They proposed a method to map the image
search problem into the well-known document search problem. This is achieved by asserting that there is an analogy between 
interesting regions in an image and words in a document. The interesting regions are referred to as "visual word" \cite{sivic2003video},
and are obtained by running K-means on all the descriptors of interesting regions. Once the problem mapping is done, standard techniques
from the document retrieval literature, namely term frequency-inverse document frequency (tf-idf) \cite{salton1983introduction, salton1988term}
can be applied for the search and retrieval tasks. In addition, the popular Average Normalized Retrieval Rank (ANRR) \cite{muller2002truth} performance criterion for document search can also be used to evaluate the performance of an image search system.

Our  implementation of Video Google follows closely with the algorithm outlined in \cite{sivic2003video}, but with minor modifications and simplifications.
The details are given in Section \ref{sec:alg}. Next, the performance of the system is shown in Section \ref{sec:perf}. Finally, discussion about the 
final system performance in relation with its algorithm are given in Section \ref{sec:concl}.

\section{An algorithm for Video Google}
\label{sec:alg}
The outline of an algorithm for Video Google is given as follows.
\begin{itemize}
\item Feature detection and descriptions are done independently for each frame in a video.
\item A tracking mechanism based on a simple video dynamical model and correlation is used to pruned out many noisy descriptors across frames.
\item The surviving descriptors are further clustered into visual words using K-means with Mahalanobis distance, effectively converts 
the image retrieval problem into the well-known document retrieval problem.
\item The standard term frequency-inverse document frequency (tf-idf) statistic  can now be used to represented any images. The relevance measure
between two images are defined to be the correlation (normalized scalar product) between their respective tf-idf statistics. This relevance measure is
then used for ranking and retrieval.
\end{itemize}

\subsection{Feature detection and description}
For each frame in a video, interesting regions, or features, are detected and described independently. 
In \cite{sivic2003video}, the type of region considered are Shaped-Adapted (SA) and Maximally Stable (MS) regions. However, in this project,
only Shape-Adapted regions are considered because Maximally Stable (MS) regions has become obsolete. 

For this detection task, the choice of detector is Difference of Gaussian (DoG) and descriptor is SIFT. These are standard, state-of-the-art choice for many
computer vision systems and thus reliable implementations are available online. This project used the VLFeat \cite{vedaldi08vlfeat} toolbox
in particular. 

\subsection{Descriptor tracking between frames}
It is worth noting that image searching in a video is different than searching in a collection of images.
Specifically, there are usually strong temporal structures between video frames, which does not exist for an arbitrary image collection.
Exploiting this structure, via a tracking mechanism, can help pruning out noisy descriptors and significantly reducing the amount of descriptors
that need consideration later.

While \cite{sivic2003video} mentioned a tracking mechanism based on "a simple constant velocity dynamical model and correlation", 
information about its detail implementation is not available. In this project, a (potentially different) tracking mechanism is proposed, based on our own 
interpretation of the original statement in \cite{sivic2003video}. That is, for every two consecutive frames, a correlation matrix between 
their descriptors, extracted independently from the previous step, is formed. 
A high-value element of the matrix 
indicates that the corresponding descriptors are likely to be the same across two frames. These descriptors are considered to be in the same track. 
All descriptors in the same track are summarized by their average. Descriptors that does not 
have any correlation to the frame before and after it are deemed noisy and pruned out.

\subsection{Clustering into visual words}
The surviving descriptors are clustered using the K-means algorithm with the Mahalanobis distance criterion, i.e.
$$
d(\bar{\mathbf{x}}_1, \bar{\mathbf{x}}_2) = \sqrt{(\bar{\mathbf{x}}_1  - \bar{\mathbf{x}}_2)^T\Sigma^{-1}
(\bar{\mathbf{x}}_1  - \bar{\mathbf{x}}_2)}
$$
where $\bar{\mathbf{x}}_i$ is the average descriptor of track $i$ and $\Sigma$ is the covariance matrix estimated from 
all the tracked  average descriptors. The use of the general Mahalanobis distance instead of the common L2 distance is to (1) decorrelated
components of the 128 dimension SIFT descriptors and (2) weight down noisy components, i.e. components with high variance, before clustering.


The K clusters form K "visual words" and effectively converted the image search problem into the well-known document search problem.

\subsection{Representing images using Tf-idf statistics}
Using a visual vocabulary of K visual words, each image can now be represented by a vector-valued statistic with dimension K called term 
frequency-inverse document frequency (tf-idf) \cite{salton1983introduction, salton1988term} $(s_1, \dots, s_i, \dots, s_K)^T$ with
$$
s_i = \text{tf}(i, d) \times \text{idf}(i, D)
$$
where 
$$
\begin{aligned}
\text{tf}(i, d) &= \frac{n_{id}}{n_d}\\
\text{idf}(i, D) &= \log\frac{N}{1+|\{d\in D: i\in d\}|}
\end{aligned}
$$
and $n_{id}$ is the number of visual word $i$ in image $d$, $n_d$ is the number of visual words in image $d$, $D$ is the set of all frames
in the video, and $N = |D|$.

Intuitively, the tf term characterizes an image by the frequencies of visual words in it. However, commonly occurred visual words among the entire
video should be weighted down for a good characterization. This is the idea behind the idf term, which down weights the frequency of a visual word
that appears in many frames of the video. The extra 1 in the numerator of idf is simply a practical adjustment to avoid a division-by-zero.

Using the tf-idf statistics as a representation, any two images can now be measured for relevance by the correlation operator. All subsequent ranking 
and retrieval are based on this relevance measure.

\section{Performance evaluation}
\label{sec:perf}
The algorithm described in Section \ref{sec:alg} is applied to a video clip of the Charade movie. The video clip has a total of 3948 frames sampling at
the standard rate of 24 frames per second. Due to (1) limited computational power (in this case, a laptop) and (2) costly labeling time, the clip is down sampled to 1 frame per second, thus leaving a total 165 frames \footnote{This is around the same number of frames that was used for performance evaluation in \cite{sivic2003video} }. Each frame is then manually labeled into a category based on their similarity. There are 
a total of 25 categories.

Feature detection and description was done using the  {\it vl\_covdet} API of VLFeat \cite{vedaldi08vlfeat}, with the flag {\it 'EstimateAffineShape'} 
set to true and the threshold {\it PeakThreshold} and {\it EdgeThreshold} both set to 5. This yields a total of 47617 descriptors for all 165 frames.

Tracking was done with a correlation threshold set to 0.8, that is, the correlation between two descriptors of two consecutive frames needs to exceed 0.8
in order to form a track. This lefts a total of 8102 tracked average descriptors. These are clustered into 256 visual words, which are then used to represent 
any image by a 256 dimension tf-idf vector.

Similarity between a given query image and all the frames in the video can be measured by correlating their respective tf-idf vectors. This is used to rank
the frames in the video for query relevance. In order to objectively evaluate the performance of our implementation, the average normalized retrieval rank (ANRR) criterion
is used \cite{muller2002truth}.
$$
\text{ANRR} = \frac{1}{NN_{rel}} \big( \sum_{i = 1}^{N_{rel}} R_i - \frac{N_{rel}(N_{rel}+1)}{2} \big) \in [0, 1)
$$
where $N_{rel}$ is the number of relevant frames for a given query image, and $R_{i} \in [1, \dots, 165]$ is the rank of the $i$th relevant frame.
The relevancy of frames are determined from labels.
Intuitively, beside the normalizing factor $NN_{rel}$, the merit of the ANRR criterion can be interpreted by the term inside the bracket. 
For a perfect retrieval, i.e.
$R_i = i, i = 1, \dots, N_{rel}$, ANRR is 0. Anything else will increase ANRR with the worst case, i.e. $R_i = i, i = N-N_{rel}+1, \dots, N$, 
approaches 1 for large N, and 0.5 being random 
retrieval \cite{muller2002truth}.

Similar to \cite{sivic2003video}, in this project, ANRR is computed for all 165 frames using each in turn  as a query image, and is shown in Figure \ref{fig:anrr}.
Even in the worst case, ANRR of the system is 0.3677, which is still better than random retrieval.

The high peaks in Figure \ref{fig:anrr} correspond to frames that do not have temporal structures in the video clips. 
These frames basically violate the assumption of the algorithm in Section \ref{sec:alg} and as a result, very few descriptors survive. 
This explains their reduction in ranking performance.


\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{ANRR}
\caption{Performance over a all 165 frames of a video clip from the movie Charade.}
\label{fig:anrr}
\end{figure}


\section{Discussion and Conclusions}
\label{sec:concl}

Because an image object is just a sub-part of an image, the system can also be demonstrated to retrieve objects in videos with 
relatively high confidence. Figures \ref{fig:selImage} and \ref{fig:selFeatExtract} shows the selection and feature extraction of an object
in frame 47. Because there are only a few descriptors extracted in that object, the retrieval performance, shown in 
Figure \ref{fig:picSearchResult} with the decreasing rank goes from left to right and up to down, 
is not perfect. Frame number 47 is actually ranked number 10.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{selImage}
\caption{Selecting an object on frame 47.}
\label{fig:selImage}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{selFeatExtract}
\caption{Extracted features from the selected object on frame 47.}
\label{fig:selFeatExtract}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{picSearchResult}
\caption{Retrieval results for the selected object on frame 47.}
\label{fig:picSearchResult}
\end{figure}

In another object selection for frame 44, more descriptors are selected and yields a perfect ranking/retrieving performance (See Figures
\ref{fig:selImage}, \ref{fig:selFeatExtract}, and \ref{fig:picSearchResult}). 
Notice that across the
all the frames in the video clip, only frame 45 and 46 are related to frame 44.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{selImage2}
\caption{Selecting an object on frame 44.}
\label{fig:selImage}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{selFeatExtract2}
\caption{Extracted features from the selected object on frame 44.}
\label{fig:selFeatExtract}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{picSearchResult2}
\caption{Retrieval results for the selected object on frame 44.}
\label{fig:picSearchResult}
\end{figure}


Unlike \cite{sivic2003video}, this project doesn't implement the stoplist idea, which makes hard decision to remove visual words that are too common 
in a video. A justification for this is that the notion of suppressing common visual words is already captured in a soft way by the idf weighting terms.
One idea that we would like to explore in future work is being able to exploit the spatial consistency to further improve the performance of the Video
Google system.

\section{Statement of individual contribution}
The project code and documentation repo can be accessed at the following URL \url{https://github.com/long0612/ECE549-proj}.

Amey Chaugule discovered the website that has a demo of \cite{sivic2003video}  at 
\url{http://www.robots.ox.ac.uk/~vgg/research/vgoogle/} and downloaded the Charade video clip that was used in this project. He also
authored some code that was not used in the project eventually. 

Long Le did everything else.

% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{finalproj}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\%begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}




% that's all folks
\end{document}


